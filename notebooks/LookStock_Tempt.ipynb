{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 1. Prepara√ß√£o\n",
        "\n"
      ],
      "metadata": {
        "id": "PTea4gOONr_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "# !pip install roboflow --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxV0y-uwDV-V",
        "outputId": "79dcd419-4379-48dc-a72f-6210e2be39d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.232-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.3.232-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.232 ultralytics-thop-2.0.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1: Montar Google Drive e Instalar Bibliotecas\n",
        "from IPython import display\n",
        "\n",
        "# display.clear_output()\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# ultralytics.checks()\n",
        "\n",
        "from IPython.display import display, Image\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "# Certifique-se de que todas as bibliotecas necess√°rias estejam instaladas, se faltar alguma!\n",
        "\n",
        "# Defini√ß√µes de Caminho (AJUSTE CONFORME SEU PROJETO)\n",
        "DRIVE_PROJECT_PATH = '/content/drive/MyDrive/Colab Notebooks/RobotPerception/LookStock'\n",
        "IMAGES_INPUT_DIR = f'{DRIVE_PROJECT_PATH}/dataset' # <--- ONDE EST√ÉO AS 412 IMAGENS\n",
        "OUTPUT_DIR = f'{DRIVE_PROJECT_PATH}/meus_resultados'\n",
        "\n",
        "MODEL_YOLO_PATH = f'{DRIVE_PROJECT_PATH}/models/best.pt'\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Garante que os diret√≥rios de sa√≠da existam\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, 'imagens_processadas'), exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, 'relatorios_individuais'), exist_ok=True)\n",
        "\n",
        "print(f\"Dispositivo de processamento: {DEVICE}\")\n",
        "\n",
        "# os.chdir('/content/drive/MyDrive/Colab Notebooks/RobotPerception/LookStock')\n",
        "# !ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWT38ec_SbLP",
        "outputId": "794cd191-dac4-44c2-a97f-23325239cb62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dispositivo de processamento: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Fun√ß√µes de Identifica√ß√£o e An√°lise"
      ],
      "metadata": {
        "id": "xBGjQfQIS300"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2: Base de Conhecimento e Planograma\n",
        "# ----------------------------------------------------------------------\n",
        "# ‚ö†Ô∏è Substitua pelos seus embeddings de refer√™ncia reais\n",
        "KNOWLEDGE_BASE = {\n",
        "  'Wonder_White': np.random.rand(2048),\n",
        "  'Sunblest_White': np.random.rand(2048),\n",
        "  'Minky_Soft': np.random.rand(2048),\n",
        "  'Countdown_White': np.random.rand(2048)\n",
        "}\n",
        "\n",
        "# C√©lula 2: Base de Conhecimento e Embeddings\n",
        "\n",
        "# ‚ö†Ô∏è SUBSTITUA PELOS SEUS DADOS REAIS\n",
        "# KNOWLEDGE_BASE = {\n",
        "#     'CocaCola_Lata': np.random.rand(2048),\n",
        "#     'Fanta_Laranja': np.random.rand(2048),\n",
        "#     'Sprite_Pet': np.random.rand(2048),\n",
        "# }\n",
        "\n",
        "PLANOGRAMA_IDEAL = {\n",
        "  'Wonder_White': 10,\n",
        "  'Sunblest_White': 8,\n",
        "  'Minky_Soft': 5,\n",
        "  'Countdown_White': 3\n",
        "}\n",
        "SIMILARITY_THRESHOLD = 0.70 # Limite para considerar um SKU identificado\n",
        "\n",
        "# Inicializa√ß√£o do Modelo de Embeddings (ResNet50)\n",
        "embedding_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "embedding_model = torch.nn.Sequential(*(list(embedding_model.children())[:-1]))\n",
        "embedding_model.to(DEVICE)\n",
        "embedding_model.eval()\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def get_embedding(image_crop_np):\n",
        "    \"\"\"Gera o vetor de embedding na GPU.\"\"\"\n",
        "    try:\n",
        "        img_pil = Image.fromarray(cv2.cvtColor(image_crop_np, cv2.COLOR_BGR2RGB))\n",
        "        img_tensor = preprocess(img_pil).unsqueeze(0).to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            embedding = embedding_model(img_tensor).cpu().numpy().flatten()\n",
        "        return embedding\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def identify_product(new_embedding, knowledge_base=KNOWLEDGE_BASE, similarity_threshold=SIMILARITY_THRESHOLD):\n",
        "    \"\"\"Compara o embedding e retorna o SKU mais prov√°vel.\"\"\"\n",
        "    if new_embedding is None: return \"SKU_DESCONHECIDO\"\n",
        "\n",
        "    max_similarity = -1\n",
        "    best_match_sku = \"SKU_DESCONHECIDO\"\n",
        "    new_embedding = new_embedding.reshape(1, -1)\n",
        "\n",
        "    for sku, ref_embedding in knowledge_base.items():\n",
        "        ref_embedding = ref_embedding.reshape(1, -1)\n",
        "        similarity = cosine_similarity(new_embedding, ref_embedding)[0][0]\n",
        "\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            best_match_sku = sku\n",
        "\n",
        "    if max_similarity > similarity_threshold:\n",
        "        return f\"{best_match_sku}\"\n",
        "    else:\n",
        "        return f\"SKU_DESCONHECIDO\""
      ],
      "metadata": {
        "id": "jfco9vjtS7Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Pipeline de Processamento em Lote"
      ],
      "metadata": {
        "id": "_Qql1RXTT49F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 3: Fun√ß√µes de Processamento em Lote\n",
        "\n",
        "def process_shelf_image(image_path, yolo_model):\n",
        "    \"\"\"Processa uma √∫nica imagem: detec√ß√£o, identifica√ß√£o e anota√ß√£o visual de Rupturas.\"\"\"\n",
        "\n",
        "    img_bgr = cv2.imread(image_path)\n",
        "    if img_bgr is None:\n",
        "        return None, None, {\"status\": \"Erro: Imagem n√£o encontrada ou inv√°lida\"}\n",
        "\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    contagem_real = {}\n",
        "    total_empty_shelves = 0\n",
        "    summary_data = {'image_file': os.path.basename(image_path)}\n",
        "\n",
        "    # 1. Executar YOLO\n",
        "    results = yolo_model.predict(source=image_path, save=False, conf=0.5, device=DEVICE.type, verbose=False)\n",
        "\n",
        "    if not results or not results[0].boxes:\n",
        "        summary_data['ruptura_vazio_direto'] = 0\n",
        "        summary_data['ruptura_planograma_unid'] = 0\n",
        "        summary_data['status_geral'] = 'Nenhuma Detec√ß√£o'\n",
        "        return img_rgb, contagem_real, summary_data\n",
        "\n",
        "    names = results[0].names\n",
        "    detected_boxes = results[0].boxes.xyxy.cpu().numpy()\n",
        "    detected_classes = results[0].boxes.cls.cpu().numpy()\n",
        "    detected_confidences = results[0].boxes.conf.cpu().numpy()\n",
        "\n",
        "    # 2. Percorrer cada detec√ß√£o e Anotar\n",
        "    for box, cls_id, conf in zip(detected_boxes, detected_classes, detected_confidences):\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        class_name = names[int(cls_id)]\n",
        "\n",
        "        # L√≥gica de Visualiza√ß√£o e Ruptura (Baseado no ShelfSight)\n",
        "        if class_name == 'emptySpaces' or class_name == 'empty_shelf': # üî¥ CLASSE DE RUPTURA\n",
        "            # Cor Vermelha Escura para RUPTURA\n",
        "            color_rgb_text = (178, 34, 34)\n",
        "            label = f\"RUPTURA {conf:.2f}\"\n",
        "            total_empty_shelves += 1\n",
        "\n",
        "        else:\n",
        "            # Produto: Tenta identificar via Embeddings\n",
        "            product_crop = img_bgr[y1:y2, x1:x2]\n",
        "            product_id = identify_product(get_embedding(product_crop))\n",
        "            contagem_real[product_id] = contagem_real.get(product_id, 0) + 1\n",
        "\n",
        "            if product_id == \"SKU_DESCONHECIDO\":\n",
        "                # Produto n√£o identificado (Laranja para revis√£o)\n",
        "                color_rgb_text = (255, 165, 0)\n",
        "                label = f\"SKU_DESC {conf:.2f}\"\n",
        "            else:\n",
        "                # Produto identificado (Verde Carregado)\n",
        "                color_rgb_text = (0, 128, 0)\n",
        "                label = f\"{product_id} {conf:.2f}\"\n",
        "\n",
        "        # Anotar a imagem\n",
        "        cv2.rectangle(img_rgb, (x1, y1), (x2, y2), color_rgb_text, 2)\n",
        "        cv2.putText(img_rgb, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color_rgb_text, 2)\n",
        "\n",
        "    # 3. An√°lise de Ruptura por Planograma (Complementar √† detec√ß√£o direta)\n",
        "    rupturas_por_planograma_unid = 0\n",
        "    for sku_esperado, ideal_count in PLANOGRAMA_IDEAL.items():\n",
        "        real_count = contagem_real.get(sku_esperado, 0)\n",
        "        if real_count < ideal_count:\n",
        "            rupturas_por_planograma_unid += (ideal_count - real_count)\n",
        "\n",
        "    # 4. Constru√ß√£o do Resumo\n",
        "    summary_data['ruptura_vazio_direto'] = total_empty_shelves\n",
        "    summary_data['ruptura_planograma_unid'] = rupturas_por_planograma_unid\n",
        "    summary_data['status_geral'] = 'Alerta' if (total_empty_shelves > 0 or rupturas_por_planograma_unid > 0) else 'OK'\n",
        "\n",
        "    return img_rgb, contagem_real, summary_data\n",
        "\n",
        "\n",
        "def process_all_images(input_dir, output_dir, model_path):\n",
        "    \"\"\"Fun√ß√£o principal para processar todas as imagens em lote.\"\"\"\n",
        "    print(\"Iniciando processamento em lote...\")\n",
        "\n",
        "    yolo_model = YOLO(model_path)\n",
        "    image_files = glob.glob(os.path.join(input_dir, '*.*'))\n",
        "    if not image_files:\n",
        "        print(f\"ERRO: Nenhuma imagem encontrada no diret√≥rio: {input_dir}\")\n",
        "        return\n",
        "\n",
        "    all_summaries = []\n",
        "\n",
        "    for i, img_path in enumerate(image_files):\n",
        "        print(f\"({i+1}/{len(image_files)}) Processando: {os.path.basename(img_path)}...\")\n",
        "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
        "\n",
        "        processed_img, real_counts, summary = process_shelf_image(img_path, yolo_model)\n",
        "\n",
        "        if processed_img is None:\n",
        "            summary['status_geral'] = 'ERRO DE ARQUIVO'\n",
        "            all_summaries.append(summary)\n",
        "            continue\n",
        "\n",
        "        # Adiciona a contagem real ao resumo\n",
        "        for sku_name in PLANOGRAMA_IDEAL.keys():\n",
        "            summary[f'Contagem_{sku_name}'] = real_counts.get(sku_name, 0)\n",
        "\n",
        "        all_summaries.append(summary)\n",
        "\n",
        "        # Salvar Imagem Processada\n",
        "        output_image_path = os.path.join(output_dir, 'imagens_processadas', f'{base_name}_processed.jpg')\n",
        "        cv2.imwrite(output_image_path, cv2.cvtColor(processed_img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    # Gerar Relat√≥rio CSV Final\n",
        "    df_summary = pd.DataFrame(all_summaries)\n",
        "    for sku_name in PLANOGRAMA_IDEAL.keys():\n",
        "        if f'Contagem_{sku_name}' not in df_summary.columns:\n",
        "            df_summary[f'Contagem_{sku_name}'] = 0\n",
        "    df_summary = df_summary.fillna(0)\n",
        "\n",
        "    final_csv_path = os.path.join(output_dir, 'RESUMO_GLOBAL_RUPTURAS.csv')\n",
        "    df_summary.to_csv(final_csv_path, index=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"‚úÖ Processamento em Lote CONCLU√çDO!\")\n",
        "    print(f\"Resumo global salvo em: {final_csv_path}\")\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "H88VuoLyT80G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 4: EXECU√á√ÉO\n",
        "# ‚ö†Ô∏è GARANTA QUE O MODELO YOLO FOI TREINADO COM A CLASSE 'emptySpaces' ‚ö†Ô∏è\n",
        "process_all_images(IMAGES_INPUT_DIR, OUTPUT_DIR, MODEL_YOLO_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEWMCSOHcwsk",
        "outputId": "f4fdbf5b-2bd2-4a55-bfab-c9e9c3438e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando processamento em lote...\n",
            "(1/18) Processando: 0c2cc2c7-243a-4194-821b-ec0682cfe42e_jpg.rf.358957deb4b032d1b6b99f586ab0000b.jpg...\n",
            "(2/18) Processando: 0e09c4a1-cbee-4c54-9b63-cf7e15ba790c_jpg.rf.58970a830cc69a3ad8c8a047fb02a930.jpg...\n",
            "(3/18) Processando: 00eb59e1-ee3d-4ea4-81be-2698eeb9018d_jpg.rf.fd8a6928713d0f71587ccb576fdba46a.jpg...\n",
            "(4/18) Processando: 25b84ba7-ac2d-4bef-bb47-b75e44a7f3e0_jpg.rf.79f9bf404ac6a1e13009639ee545047b.jpg...\n",
            "(5/18) Processando: 1a5a8f13-80bf-404c-915a-ec5bd52c3453_jpg.rf.acc4297a6c599a316d50ca95019be039.jpg...\n",
            "(6/18) Processando: 25e6a0f5-a5e0-445e-becf-59e4d3c3e438_jpg.rf.4e47f4870e407e411e63f5b629d23062.jpg...\n",
            "(7/18) Processando: 1ea3cd9f-6b24-46c6-906d-7b97651c276f_jpg.rf.0b1c35b682684a6293422ccc6fc2af89.jpg...\n",
            "(8/18) Processando: 58c5cd28-70ad-4e7b-a458-cd0d6400f53f_jpg.rf.713f334af9000a2bca4a17ca288d8f2e.jpg...\n",
            "(9/18) Processando: 58b6a885-7ba0-490d-9a3b-ea98bd99dc65_jpg.rf.020235590dd3698b92f0bd59f0739b14.jpg...\n",
            "(10/18) Processando: 6d19a01c-9065-43d2-aa4e-b8f64581ab16_jpg.rf.dad6232fc0c23aa46ecd0d9ca1a2e131.jpg...\n",
            "(11/18) Processando: 6c3eadc6-86f9-42a5-b829-ab896892be84_jpg.rf.1c845282a38d722497c5d9b24469f23d.jpg...\n",
            "(12/18) Processando: 6c578f6b-8a93-4d5a-9689-e93ec20cc5cd_jpg.rf.4ec1483b7411039b686f6ee5d2156c90.jpg...\n",
            "(13/18) Processando: 6c10a547-3bf1-4ed9-9f78-e6fbf677ca7c_jpg.rf.9084279c28329a1430d164d93b6b8c0f.jpg...\n",
            "(14/18) Processando: 7bf5ad34-7784-41a6-a48c-23e07f308098_jpg.rf.f06b3e80fd22afc73b5723ae55c59fd9.jpg...\n",
            "(15/18) Processando: 7e390ea7-811d-4e5c-9d33-a2aab2b44e19_jpg.rf.e36b341536e8eb6f31e32d65b8537b84.jpg...\n",
            "(16/18) Processando: bb354528-c5f2-40a1-9854-4bd8e6c3f149_jpg.rf.e2521fcd43bf36d40753554a3a48777c.jpg...\n",
            "(17/18) Processando: bcbbfa2b-d8ad-4343-b450-a127f5b9a845_jpg.rf.0da02c06df4aa048aaaf771368988bf3.jpg...\n",
            "(18/18) Processando: ff6abc46-541d-4832-b206-b9b9fd4d1265_jpg.rf.19b854126a7139294482c17f769df0f5.jpg...\n",
            "\n",
            "==================================================\n",
            "‚úÖ Processamento em Lote CONCLU√çDO!\n",
            "Resumo global salvo em: /content/drive/MyDrive/Colab Notebooks/RobotPerception/LookStock/meus_resultados/RESUMO_GLOBAL_RUPTURAS.csv\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}